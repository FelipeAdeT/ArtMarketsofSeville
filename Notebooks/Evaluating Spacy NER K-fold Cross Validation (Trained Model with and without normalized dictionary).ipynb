{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the results of Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of training (and its evaluation) will depend on how the data was split into training and testing sets. In this worksheet, we use repeated random subsampling to assess the performance of our trained model.\n",
    "\n",
    "According to wikipedia: \n",
    ">In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,[11] but in general k remains an unfixed parameter.\n",
    "\n",
    ">For example, setting k = 2 results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets d0 and d1, so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on d0 and validate on d1, followed by training on d1 and validating on d0.\n",
    "\n",
    ">When k = n (the number of observations), the k-fold cross-validation is exactly the leave-one-out cross-validation.[12]\n",
    "\n",
    ">In stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In the case of binary classification, this means that each fold contains roughly the same proportions of the two types of class labels.\n",
    "\n",
    ">In repeated cross-validation the data is randomly split into k folds several times. The performance of the model can thereby be averaged over several runs, but this is rarely desirable in practice.[13]\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish \n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "from collections import defaultdict, Counter\n",
    "from spacy.attrs import ORTH\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.language import GoldParse\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Dataturks to Spacy Format\n",
    "\n",
    "TAGGED_DATA = np.array(convert_dataturks_to_spacy(\"/Users/Felipe/Documents/Research/NPL/SevillianPaintersNPL/seville painters test 2-3.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define evaluate function\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for sents, ents in examples:\n",
    "        doc_gold = ner_model.make_doc(sents)\n",
    "        gold = GoldParse(doc_gold, entities=ents['entities'])\n",
    "        pred_value = ner_model(sents)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy Model\n",
    "nlp= spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters of k-fold split (5 batches, with random shuffle, set seed = 2)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "split= kf.split(TAGGED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data = pd.DataFrame(columns=columns)\n",
    "eval_data = eval_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 28763.270657695703}\n",
      "Losses {'ner': 25000.547394746973}\n",
      "Losses {'ner': 21981.59359280667}\n",
      "Losses {'ner': 17886.673262105265}\n",
      "Losses {'ner': 20244.352677792045}\n",
      "Losses {'ner': 24499.74955254048}\n",
      "Losses {'ner': 25568.97612799052}\n",
      "Losses {'ner': 27677.37665426545}\n",
      "Losses {'ner': 28639.05391213298}\n",
      "Losses {'ner': 29057.6745602265}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 28284.365855179843}\n",
      "Losses {'ner': 27317.948948207977}\n",
      "Losses {'ner': 26979.25418621886}\n",
      "Losses {'ner': 23497.794500031094}\n",
      "Losses {'ner': 18448.238092696294}\n",
      "Losses {'ner': 19399.105394817656}\n",
      "Losses {'ner': 21368.216864025337}\n",
      "Losses {'ner': 23970.236314468086}\n",
      "Losses {'ner': 26129.32080714032}\n",
      "Losses {'ner': 27405.21348118782}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 35633.34429153843}\n",
      "Losses {'ner': 32028.725445642172}\n",
      "Losses {'ner': 22923.35013491605}\n",
      "Losses {'ner': 17980.818063028128}\n",
      "Losses {'ner': 18253.946180973053}\n",
      "Losses {'ner': 17280.585044681793}\n",
      "Losses {'ner': 17710.128905594174}\n",
      "Losses {'ner': 18595.123207718134}\n",
      "Losses {'ner': 22507.744688019156}\n",
      "Losses {'ner': 23510.849381119013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 27091.85743642479}\n",
      "Losses {'ner': 22025.774144502764}\n",
      "Losses {'ner': 15901.80189734892}\n",
      "Losses {'ner': 12806.28643740952}\n",
      "Losses {'ner': 10244.279531438795}\n",
      "Losses {'ner': 10797.844315719667}\n",
      "Losses {'ner': 10856.318150190167}\n",
      "Losses {'ner': 11154.773612475054}\n",
      "Losses {'ner': 11250.234395478881}\n",
      "Losses {'ner': 11382.200327911145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 36625.64222108561}\n",
      "Losses {'ner': 38606.43633953873}\n",
      "Losses {'ner': 37973.322053941694}\n",
      "Losses {'ner': 27511.949096575616}\n",
      "Losses {'ner': 22411.499419410015}\n",
      "Losses {'ner': 21987.13741102442}\n",
      "Losses {'ner': 24146.043497385457}\n",
      "Losses {'ner': 25628.12248749286}\n",
      "Losses {'ner': 27520.854878157377}\n",
      "Losses {'ner': 28561.475986121222}\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in split:\n",
    "    \n",
    "    #Generate training and test data\n",
    "    traindata = TAGGED_DATA[train_index]\n",
    "    testdata = TAGGED_DATA[test_index]\n",
    "    \n",
    "    #Load the model to be trained (save separately, because we do not want to repeatedly retrain the same model)\n",
    "    nlp1 = deepcopy(nlp)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp1.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    ner.add_label(\"OBJ\")\n",
    "    ner.add_label(\"MON\")\n",
    "    ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp1.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp1.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(traindata)\n",
    "            batches = minibatch(traindata, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp1.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    results = evaluate(nlp1,testdata)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    oblist = list(ev_obj[0].values())\n",
    "    newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data=eval_data.append(newrow1,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow2,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow3,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow4,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow5,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ents_p     ents_r     ents_f label\n",
      "0     0.000000   0.000000   0.000000  DATE\n",
      "1   100.000000   1.587302   3.125000   MON\n",
      "2     0.000000   0.000000   0.000000   OBJ\n",
      "3    66.666667   8.000000  14.285714   ORG\n",
      "4    82.727273  80.888889  81.797753   PER\n",
      "5    71.532847  70.503597  71.014493   LOC\n",
      "6     0.000000   0.000000   0.000000  DATE\n",
      "7    80.952381  36.170213  50.000000   MON\n",
      "8     0.000000   0.000000   0.000000   OBJ\n",
      "9    60.000000   7.142857  12.765957   ORG\n",
      "10   82.710280  83.098592  82.903981   PER\n",
      "11   84.251969  76.978417  80.451128   LOC\n",
      "12    0.000000   0.000000   0.000000  DATE\n",
      "13   33.333333   1.149425   2.222222   MON\n",
      "14    0.000000   0.000000   0.000000   OBJ\n",
      "15   85.714286   9.090909  16.438356   ORG\n",
      "16   83.482143  74.501992  78.736842   PER\n",
      "17   77.205882  61.046512  68.181818   LOC\n",
      "18    0.000000   0.000000   0.000000  DATE\n",
      "19   84.615385  22.916667  36.065574   MON\n",
      "20    0.000000   0.000000   0.000000   OBJ\n",
      "21   73.684211  25.000000  37.333333   ORG\n",
      "22   87.500000  84.232365  85.835095   PER\n",
      "23   88.679245  60.645161  72.030651   LOC\n",
      "24    0.000000   0.000000   0.000000  DATE\n",
      "25   60.000000   4.109589   7.692308   MON\n",
      "26    0.000000   0.000000   0.000000   OBJ\n",
      "27    0.000000   0.000000   0.000000   ORG\n",
      "28   83.884298  82.857143  83.367556   PER\n",
      "29   77.631579  72.839506  75.159236   LOC\n"
     ]
    }
   ],
   "source": [
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uas': 0.0, 'las': 0.0, 'ents_p': 83.10626702997274, 'ents_r': 48.41269841269841, 'ents_f': 61.18355065195586, 'ents_per_type': {'PER': {'p': 84.38818565400844, 'r': 81.63265306122449, 'f': 82.98755186721992}, 'MON': {'p': 66.66666666666666, 'r': 5.47945205479452, 'f': 10.126582278481013}, 'LOC': {'p': 81.81818181818183, 'r': 61.111111111111114, 'f': 69.96466431095408}, 'ORG': {'p': 66.66666666666666, 'r': 3.8461538461538463, 'f': 7.272727272727273}, 'DATE': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'OBJ': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'tags_acc': 0.0, 'token_acc': 100.0, 'textcat_score': 0.0, 'textcats_per_cat': {}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure mean and standard deviation of f, p and r scores for each label \n",
    "a = eval_data.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model trained with EMS dictionary\n",
    "\n",
    "#Generate empty dictionary for storing evaluation results of different trials\n",
    "d2 = {}\n",
    "\n",
    "#Loop 10 times\n",
    "for x in range(0,11):\n",
    "    \n",
    "    random.shuffle(TAGGED_DATA)\n",
    "    train_data = TAGGED_DATA[:326]\n",
    "    test_data = TAGGED_DATA[326:]\n",
    "    \n",
    "    #Load the model to be trained\n",
    "    nlp2 = nlp\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp2.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    ner.add_label(\"OBJ\")\n",
    "    ner.add_label(\"MON\")\n",
    "    ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp2.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp2.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp1.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "            \n",
    "    results = evaluate(nlp2,test_data)\n",
    "    d2[x] = pd.DataFrame(results)\n",
    "    \n",
    "    \n",
    "eval_data2 = pd.DataFrame(columns=columns)\n",
    "eval_data2 = eval_data2.fillna(0)\n",
    "\n",
    "for x in d2:\n",
    "    ev_date= d2[x].loc['DATE','ents_per_type']\n",
    "    ev_loc= d2[x].loc['LOC','ents_per_type']\n",
    "    ev_mon= d2[x].loc['MON','ents_per_type']\n",
    "    ev_obj= d2[x].loc['OBJ','ents_per_type']\n",
    "    ev_org= d2[x].loc['ORG','ents_per_type']\n",
    "    ev_per= d2[x].loc['PER','ents_per_type']\n",
    "    newrow1={'ents_p':ev_date['p'],'ents_r':ev_date['r'],'ents_f':ev_date['f'],'label':'DATE','trial':x}\n",
    "    newrow2={'ents_p':ev_loc['p'],'ents_r':ev_loc['r'],'ents_f':ev_loc['f'],'label':'LOC','trial':x}\n",
    "    newrow3={'ents_p':ev_mon['p'],'ents_r':ev_mon['r'],'ents_f':ev_mon['f'],'label':'MON','trial':x}\n",
    "    newrow4={'ents_p':ev_obj['p'],'ents_r':ev_obj['r'],'ents_f':ev_obj['f'],'label':'OBJ','trial':x}\n",
    "    newrow5={'ents_p':ev_org['p'],'ents_r':ev_org['r'],'ents_f':ev_org['f'],'label':'ORG','trial':x}\n",
    "    newrow6={'ents_p':ev_per['p'],'ents_r':ev_per['r'],'ents_f':ev_per['f'],'label':'PER','trial':x}\n",
    "    eval_data2=eval_data2.append(newrow1,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow2,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow3,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow4,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow5,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= eval_data2.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f                ents_p                ents_r           \n",
      "            mean        std       mean        std       mean        std\n",
      "label                                                                  \n",
      "DATE    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000\n",
      "LOC    73.367465   4.679829  79.860304   6.678700  68.402639   7.278991\n",
      "MON    19.821021  21.853128  71.780220  25.798075  13.186639  15.690364\n",
      "OBJ     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000\n",
      "ORG    16.164672  13.458877  57.213033  33.369487   9.846753   9.190651\n",
      "PER    82.528246   2.583081  84.060799   1.987227  81.115796   3.888144\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.67546666666667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53.52-1.96*19.66/(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.36453333333334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53.52+1.96*19.66/(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.26866666666668"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96.06+1.96*1.85/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.85133333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96.06-1.96*1.85/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.9984"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "93.39-1.96*2.13/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.7816"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "93.39+1.96*2.13/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
