{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the results of Training (K-fold Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of training (and its evaluation) will depend on how the data was split into training and testing sets. In this worksheet, we use repeated random subsampling to assess the performance of our trained model.\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics)): \n",
    ">In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,[11] but in general k remains an unfixed parameter.\n",
    "\n",
    "More information available [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n",
    "\n",
    "For us, measuring performance with different samples is important because of the wide variation in the data: texts vary widely in length, in type, and in transcription conventions. We cannot tell clearly whether the performance of the model, when measured only once, reflects an improvement in the model through training or whether it is the result of the division into training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish \n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "from collections import defaultdict, Counter\n",
    "from spacy.attrs import ORTH\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.language import GoldParse\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Tagged Data from JSON file\n",
    "with open('TaggedData_SF.json', 'r', encoding='utf-8') as fp2:\n",
    "    TAGGED_DATA = json.load(fp2)\n",
    "    \n",
    "TD_np = np.array(TAGGED_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has a built-in function for evaluating a model's performance using the [command line](https://spacy.io/api/cli#evaluate), but alternatively you can define a function like the one below. It takes the NER model and examples that you input and returns several metrics:\n",
    "        - UAS (Unlabelled Attachment Score) \n",
    "        - LAS (Labelled Attachment Score)\n",
    "        - ents_p\n",
    "        - ents_r\n",
    "        - ents_f\n",
    "        - tags_acc\n",
    "        - token_acc\n",
    "\n",
    "[According](https://github.com/explosion/spaCy/issues/2405) to one of the creators of Spacy, \n",
    ">The UAS and LAS are standard metrics to evaluate dependency parsing. UAS is the proportion of tokens whose head has been correctly assigned, LAS is the proportion of tokens whose head has been correctly assigned with the right dependency label (subject, object, etc).\n",
    ">ents_p, ents_r, ents_f are the precision, recall and fscore for the NER task.\n",
    ">tags_acc is the POS tagging accuracy.\n",
    ">token_acc seems to be the precision for token segmentation.\n",
    "\n",
    "The key metrics for this task are the precision, recall and f-score.\n",
    "**Precision** (ents_p) is the ratio of correctly-labeled entities out of all the entities labeled. (True Positive/(True Positive+False Positive)).\n",
    "**Recall**  (ents_r) is the ratio of correctly-labeled entities out of all true entities (True Positive/(True Positive+False Negative)). The F-score is the mean of both values.  \n",
    "\n",
    "These metrics all appear averaged out through all the entity types (labels) and then detailed for each label in particular. We want these values to be as close as possible to 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the evaluate function\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for sents, ents in examples:\n",
    "        doc_gold = ner_model.make_doc(sents)\n",
    "        gold = GoldParse(doc_gold, entities=ents['entities'])\n",
    "        pred_value = ner_model(sents)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the spacy model and split the data into the n batches that we will use in the cross-validation. In this procedure, we will train the model n-1 times, reserving one fold for testing the model each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy Model\n",
    "nlp= spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters of k-fold split (5 batches, with random shuffle, set seed = 2)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "split= kf.split(TD_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a dataframe to store the results of each training, with the evaluation scores for each label type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data = pd.DataFrame(columns=columns)\n",
    "eval_data = eval_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the training loop for each set of training data excluding one fold and evaluate the results, storing these in our dataframe. We are using a copy of the NLP model because we want the training to start afresh for each set of training data. Otherwise, the model would be trained on all the data including the test data, leading to the model overperforming on the tagged data compared to new samples that we are interested in tagging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 30721.031245049875}\n",
      "Losses {'ner': 35812.11033296434}\n",
      "Losses {'ner': 42480.974824460296}\n",
      "Losses {'ner': 33017.01195325819}\n",
      "Losses {'ner': 27781.430804745098}\n",
      "Losses {'ner': 20094.126222006045}\n",
      "Losses {'ner': 20143.287001546472}\n",
      "Losses {'ner': 23534.748939459212}\n",
      "Losses {'ner': 25472.321697831154}\n",
      "Losses {'ner': 26433.625859245658}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 33503.69745281518}\n",
      "Losses {'ner': 21147.750624527725}\n",
      "Losses {'ner': 15577.639918600704}\n",
      "Losses {'ner': 13817.42245296805}\n",
      "Losses {'ner': 16201.630242561194}\n",
      "Losses {'ner': 20305.903943861354}\n",
      "Losses {'ner': 23259.951532332227}\n",
      "Losses {'ner': 25911.089790869504}\n",
      "Losses {'ner': 27671.270382288523}\n",
      "Losses {'ner': 29422.120310395956}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29816.778590951126}\n",
      "Losses {'ner': 22040.97248751149}\n",
      "Losses {'ner': 18571.245137903257}\n",
      "Losses {'ner': 12753.101068754551}\n",
      "Losses {'ner': 12725.461273144523}\n",
      "Losses {'ner': 11771.969168598589}\n",
      "Losses {'ner': 11694.639036175777}\n",
      "Losses {'ner': 13953.93449570646}\n",
      "Losses {'ner': 15851.748594797857}\n",
      "Losses {'ner': 19931.722536871108}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 31870.99545417662}\n",
      "Losses {'ner': 37097.16218843594}\n",
      "Losses {'ner': 22824.430444302758}\n",
      "Losses {'ner': 18109.200683364295}\n",
      "Losses {'ner': 18200.57962171536}\n",
      "Losses {'ner': 21151.260026738048}\n",
      "Losses {'ner': 22151.514805567043}\n",
      "Losses {'ner': 25604.13780531753}\n",
      "Losses {'ner': 26728.886173009872}\n",
      "Losses {'ner': 28074.35024024546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 33761.27361989506}\n",
      "Losses {'ner': 36317.208633122435}\n",
      "Losses {'ner': 45372.61358036703}\n",
      "Losses {'ner': 50140.59016311784}\n",
      "Losses {'ner': 47966.425998374965}\n",
      "Losses {'ner': 31716.675456080302}\n",
      "Losses {'ner': 28185.290889454925}\n",
      "Losses {'ner': 26854.486466115366}\n",
      "Losses {'ner': 24467.148040562693}\n",
      "Losses {'ner': 14209.33681134712}\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in split:\n",
    "    \n",
    "    #Generate training and test data\n",
    "    traindata = TD_np[train_index]\n",
    "    testdata = TD_np[test_index]\n",
    "    \n",
    "    #Load the model to be trained (save separately, because we do not want to repeatedly retrain the same model)\n",
    "    nlp1 = deepcopy(nlp)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp1.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    ner.add_label(\"OBJ\")\n",
    "    ner.add_label(\"MON\")\n",
    "    ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp1.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp1.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(traindata)\n",
    "            batches = minibatch(traindata, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp1.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    results = evaluate(nlp1,testdata)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    oblist = list(ev_obj[0].values())\n",
    "    newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data=eval_data.append(newrow1,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow2,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow3,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow4,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow5,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the contents of our evaluation dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ents_p     ents_r     ents_f label\n",
      "0     0.000000   0.000000   0.000000  DATE\n",
      "1    80.000000   4.705882   8.888889   MON\n",
      "2     0.000000   0.000000   0.000000   OBJ\n",
      "3    50.000000   4.000000   7.407407   ORG\n",
      "4    86.065574  83.333333  84.677419   PER\n",
      "5    88.461538  64.335664  74.493927   LOC\n",
      "6     0.000000   0.000000   0.000000  DATE\n",
      "7    50.000000   1.265823   2.469136   MON\n",
      "8     0.000000   0.000000   0.000000   OBJ\n",
      "9    84.615385  16.666667  27.848101   ORG\n",
      "10   83.018868  80.586081  81.784387   PER\n",
      "11   73.006135  68.000000  70.414201   LOC\n",
      "12    0.000000   0.000000   0.000000  DATE\n",
      "13   40.000000   4.545455   8.163265   MON\n",
      "14    0.000000   0.000000   0.000000   OBJ\n",
      "15  100.000000   5.714286  10.810811   ORG\n",
      "16   83.486239  84.259259  83.870968   PER\n",
      "17   83.561644  82.432432  82.993197   LOC\n",
      "18    0.000000   0.000000   0.000000  DATE\n",
      "19   66.666667   3.076923   5.882353   MON\n",
      "20    0.000000   0.000000   0.000000   OBJ\n",
      "21   75.000000   5.084746   9.523810   ORG\n",
      "22   84.210526  81.218274  82.687339   PER\n",
      "23   76.576577  59.859155  67.193676   LOC\n",
      "24    0.000000   0.000000   0.000000  DATE\n",
      "25    0.000000   0.000000   0.000000   MON\n",
      "26    0.000000   0.000000   0.000000   OBJ\n",
      "27   52.631579  17.857143  26.666667   ORG\n",
      "28   79.253112  80.590717  79.916318   PER\n",
      "29   86.363636  71.698113  78.350515   LOC\n"
     ]
    }
   ],
   "source": [
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we can create estimates of performance averaged over all the trials, providing a better estimate of each measurement with its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure mean and standard deviation of f, p and r scores for each label \n",
    "a = eval_data.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f               ents_p                ents_r          \n",
      "            mean       std       mean        std       mean       std\n",
      "label                                                                \n",
      "DATE    0.000000  0.000000   0.000000   0.000000   0.000000  0.000000\n",
      "LOC    74.689103  6.262153  81.593906   6.570961  69.265073  8.568399\n",
      "MON     5.080729  3.784209  47.333333  30.586853   2.718817  2.056479\n",
      "OBJ     0.000000  0.000000   0.000000   0.000000   0.000000  0.000000\n",
      "ORG    16.451359  9.947839  72.449393  21.273657   9.864568  6.793646\n",
      "PER    82.587286  1.857769  83.206864   2.496253  81.997533  1.693985\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the different labels perform consistently at the levels printed above. The PER and LOC labels are perhaps the most useful, whereas the others can still be improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Spelling Normalization\n",
    "\n",
    "We can apply the evaluation above to a model trained with text whose spelling has been normalized, thus evaluating whether the inclusion of a normalization dictionary improves training results.\n",
    "\n",
    "To apply the spelling normalization, we create a pipeline component that modifies the NORM attribute of each token according to a dictionary we provide. Spacy does not modify any text supplied permanently, this is the way they provide for correcting for spelling variation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Norm Exceptions from JSON file\n",
    "with open('normalizeddict.json', 'r', encoding='utf-8') as fp3:\n",
    "    NORM_EXCEPTIONS = json.load(fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "nlp2= spacy.load('es_core_news_md')\n",
    "\n",
    "#Define and add pipeline component that updates .norm attribute\n",
    "\n",
    "def add_custom_norms(doc):\n",
    "    for token in doc:\n",
    "        if token.text in NORM_EXCEPTIONS:\n",
    "            token.norm_ = NORM_EXCEPTIONS[token.text]\n",
    "    return doc\n",
    "\n",
    "#Add component to the pipeline\n",
    "\n",
    "nlp2.add_pipe(add_custom_norms, first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a new blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data2 = pd.DataFrame(columns=columns)\n",
    "eval_data2 = eval_data2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 27369.295011232218}\n",
      "Losses {'ner': 25288.68191463001}\n",
      "Losses {'ner': 22012.064205526956}\n",
      "Losses {'ner': 20950.277915531755}\n",
      "Losses {'ner': 15612.289248465873}\n",
      "Losses {'ner': 14711.363196730337}\n",
      "Losses {'ner': 16880.3947647771}\n",
      "Losses {'ner': 17808.167656041456}\n",
      "Losses {'ner': 19775.498188718222}\n",
      "Losses {'ner': 21840.016411602497}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 30200.0166880758}\n",
      "Losses {'ner': 32703.116319365163}\n",
      "Losses {'ner': 25241.50693244629}\n",
      "Losses {'ner': 24407.429937910696}\n",
      "Losses {'ner': 22122.65288857594}\n",
      "Losses {'ner': 22206.00163769722}\n",
      "Losses {'ner': 22984.677610472776}\n",
      "Losses {'ner': 25015.930037163198}\n",
      "Losses {'ner': 24900.114258908667}\n",
      "Losses {'ner': 25856.664362482727}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 26335.389584492088}\n",
      "Losses {'ner': 22103.978002444463}\n",
      "Losses {'ner': 18943.524095241548}\n",
      "Losses {'ner': 15619.313920508259}\n",
      "Losses {'ner': 15434.403349751716}\n",
      "Losses {'ner': 16787.341459652074}\n",
      "Losses {'ner': 20236.44105457893}\n",
      "Losses {'ner': 22059.997543136327}\n",
      "Losses {'ner': 24899.461682455614}\n",
      "Losses {'ner': 25841.326803692617}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 31956.094522983014}\n",
      "Losses {'ner': 34606.20847437257}\n",
      "Losses {'ner': 25913.667436758915}\n",
      "Losses {'ner': 18937.027232236862}\n",
      "Losses {'ner': 19383.52967597225}\n",
      "Losses {'ner': 19120.842931861873}\n",
      "Losses {'ner': 19254.03517245932}\n",
      "Losses {'ner': 21625.609265983105}\n",
      "Losses {'ner': 23578.9522626698}\n",
      "Losses {'ner': 26010.224218841642}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 37095.27128314915}\n",
      "Losses {'ner': 47508.65459043867}\n",
      "Losses {'ner': 37057.82105511987}\n",
      "Losses {'ner': 31642.729106775874}\n",
      "Losses {'ner': 25610.1483249697}\n",
      "Losses {'ner': 23099.846375429377}\n",
      "Losses {'ner': 16554.86995130687}\n",
      "Losses {'ner': 13155.681421290585}\n",
      "Losses {'ner': 10452.325621203985}\n",
      "Losses {'ner': 11027.603701612912}\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Model trained with EMS dictionary\n",
    "\n",
    "for train_index, test_index in split:\n",
    "    \n",
    "    #Generate training and test data\n",
    "    traindata = TD_np[train_index]\n",
    "    testdata = TD_np[test_index]\n",
    "    \n",
    "    #Load the model to be trained (save separately, because we do not want to repeatedly retrain the same model)\n",
    "    nlp3 = deepcopy(nlp2)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp3.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    ner.add_label(\"OBJ\")\n",
    "    ner.add_label(\"MON\")\n",
    "    ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp3.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp3.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(traindata)\n",
    "            batches = minibatch(traindata, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp3.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    results = evaluate(nlp3,testdata)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    oblist = list(ev_obj[0].values())\n",
    "    newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data2=eval_data.append(newrow1,ignore_index=True)\n",
    "    eval_data2=eval_data.append(newrow2,ignore_index=True)\n",
    "    eval_data2=eval_data.append(newrow3,ignore_index=True)\n",
    "    eval_data2=eval_data.append(newrow4,ignore_index=True)\n",
    "    eval_data2=eval_data.append(newrow5,ignore_index=True)\n",
    "    eval_data2=eval_data.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= eval_data2.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the statistics for the training with (b) and without (a) spelling normalization. As can be seen, there is a slight improvement on most measurements (as well as a reduction in variability) when we normalize spelling. \n",
    "\n",
    "This measurement provides really poor performance of the DATE and OBJ labels; this must be reviewed, but may be because of the way the data was shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f                ents_p                ents_r           \n",
      "            mean        std       mean        std       mean        std\n",
      "label                                                                  \n",
      "DATE    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000\n",
      "LOC    73.367465   4.679829  79.860304   6.678700  68.402639   7.278991\n",
      "MON    19.821021  21.853128  71.780220  25.798075  13.186639  15.690364\n",
      "OBJ     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000\n",
      "ORG    16.164672  13.458877  57.213033  33.369487   9.846753   9.190651\n",
      "PER    82.528246   2.583081  84.060799   1.987227  81.115796   3.888144\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f               ents_p                ents_r          \n",
      "            mean       std       mean        std       mean       std\n",
      "label                                                                \n",
      "DATE    0.000000  0.000000   0.000000   0.000000   0.000000  0.000000\n",
      "LOC    74.051074  5.814992  80.465784   6.494457  68.936827  7.705870\n",
      "MON     5.080729  3.784209  47.333333  30.586853   2.718817  2.056479\n",
      "OBJ     0.000000  0.000000   0.000000   0.000000   0.000000  0.000000\n",
      "ORG    16.451359  9.947839  72.449393  21.273657   9.864568  6.793646\n",
      "PER    82.587286  1.857769  83.206864   2.496253  81.997533  1.693985\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
