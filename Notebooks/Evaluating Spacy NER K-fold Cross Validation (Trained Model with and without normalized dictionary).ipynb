{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the results of Training (K-fold Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of training (and its evaluation) will depend on how the data was split into training and testing sets. In this worksheet, we use repeated random subsampling to assess the performance of our trained model.\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics)): \n",
    ">In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,[11] but in general k remains an unfixed parameter.\n",
    "\n",
    "More information available [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n",
    "\n",
    "For us, measuring performance with different samples is important because of the wide variation in the data: texts vary widely in length, in type, and in transcription conventions. We cannot tell clearly whether the performance of the model, when measured only once, reflects an improvement in the model through training or whether it is the result of the division into training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish \n",
    "from spacy.scorer import Scorer\n",
    "from spacy.language import GoldParse\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Tagged Data from JSON file\n",
    "with open('TaggedData_SF.json', 'r', encoding='utf-8') as fp2:\n",
    "    TAGGED_DATA = json.load(fp2)\n",
    "    \n",
    "TD_np = np.array(TAGGED_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has a built-in function for evaluating a model's performance using the [command line](https://spacy.io/api/cli#evaluate), but alternatively you can define a function like the one below. It takes the NER model and examples that you input and returns several metrics:\n",
    "        - UAS (Unlabelled Attachment Score) \n",
    "        - LAS (Labelled Attachment Score)\n",
    "        - ents_p\n",
    "        - ents_r\n",
    "        - ents_f\n",
    "        - tags_acc\n",
    "        - token_acc\n",
    "\n",
    "[According](https://github.com/explosion/spaCy/issues/2405) to one of the creators of Spacy, \n",
    ">The UAS and LAS are standard metrics to evaluate dependency parsing. UAS is the proportion of tokens whose head has been correctly assigned, LAS is the proportion of tokens whose head has been correctly assigned with the right dependency label (subject, object, etc).\n",
    ">ents_p, ents_r, ents_f are the precision, recall and fscore for the NER task.\n",
    ">tags_acc is the POS tagging accuracy.\n",
    ">token_acc seems to be the precision for token segmentation.\n",
    "\n",
    "The key metrics for this task are the precision, recall and f-score.\n",
    "**Precision** (ents_p) is the ratio of correctly-labeled entities out of all the entities labeled. (True Positive/(True Positive+False Positive)).\n",
    "**Recall**  (ents_r) is the ratio of correctly-labeled entities out of all true entities (True Positive/(True Positive+False Negative)). The F-score is the mean of both values.  \n",
    "\n",
    "These metrics all appear averaged out through all the entity types (labels) and then detailed for each label in particular. We want these values to be as close as possible to 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the evaluate function\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for sents, ents in examples:\n",
    "        doc_gold = ner_model.make_doc(sents)\n",
    "        gold = GoldParse(doc_gold, entities=ents['entities'])\n",
    "        pred_value = ner_model(sents)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the spacy model and split the data into the n batches that we will use in the cross-validation. In this procedure, we will train the model n-1 times, reserving one fold for testing the model each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy Model\n",
    "nlp= spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters of k-fold split (5 batches, with random shuffle, set seed = 2)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "split= kf.split(TD_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a dataframe to store the results of each training, with the evaluation scores for each label type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data = pd.DataFrame(columns=columns)\n",
    "eval_data = eval_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the training loop for each set of training data excluding one fold and evaluate the results, storing these in our dataframe. We are using a copy of the NLP model because we want the training to start afresh for each set of training data. Otherwise, the model would be trained on all the data including the test data, leading to the model overperforming on the tagged data compared to new samples that we are interested in tagging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29990.3521411966}\n",
      "Losses {'ner': 27886.84531629397}\n",
      "Losses {'ner': 19379.183527336078}\n",
      "Losses {'ner': 23267.713834718776}\n",
      "Losses {'ner': 22871.687328162086}\n",
      "Losses {'ner': 24932.390145298734}\n",
      "Losses {'ner': 26685.45910784602}\n",
      "Losses {'ner': 26078.608320454136}\n",
      "Losses {'ner': 26746.972224920988}\n",
      "Losses {'ner': 27233.27111905627}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 40258.40534344755}\n",
      "Losses {'ner': 38347.65755278419}\n",
      "Losses {'ner': 28128.2670029179}\n",
      "Losses {'ner': 26443.775922638204}\n",
      "Losses {'ner': 23749.037599277814}\n",
      "Losses {'ner': 22436.196288708597}\n",
      "Losses {'ner': 23618.400182686746}\n",
      "Losses {'ner': 25132.60542197805}\n",
      "Losses {'ner': 26051.640869945288}\n",
      "Losses {'ner': 26891.268652012222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 26593.121696714035}\n",
      "Losses {'ner': 21575.30916965817}\n",
      "Losses {'ner': 18491.861198539922}\n",
      "Losses {'ner': 18741.562300101556}\n",
      "Losses {'ner': 18413.864992712763}\n",
      "Losses {'ner': 19239.24104174958}\n",
      "Losses {'ner': 22582.034950674977}\n",
      "Losses {'ner': 25981.066629868}\n",
      "Losses {'ner': 27067.824368900387}\n",
      "Losses {'ner': 28114.243262693286}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 36249.6091657142}\n",
      "Losses {'ner': 39667.303020850275}\n",
      "Losses {'ner': 35393.552207695386}\n",
      "Losses {'ner': 33965.30115976177}\n",
      "Losses {'ner': 28090.060746957046}\n",
      "Losses {'ner': 27077.88383237197}\n",
      "Losses {'ner': 26646.217350698076}\n",
      "Losses {'ner': 27135.614880967885}\n",
      "Losses {'ner': 29054.58570339717}\n",
      "Losses {'ner': 28718.937085229903}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 25819.953981815663}\n",
      "Losses {'ner': 22649.607092659404}\n",
      "Losses {'ner': 19590.493973777015}\n",
      "Losses {'ner': 14764.599927769552}\n",
      "Losses {'ner': 9349.48336654413}\n",
      "Losses {'ner': 8263.741737522445}\n",
      "Losses {'ner': 8986.926781303526}\n",
      "Losses {'ner': 9863.53484527146}\n",
      "Losses {'ner': 10335.228494862953}\n",
      "Losses {'ner': 10955.456667964922}\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in split:\n",
    "    \n",
    "    #Generate training and test data\n",
    "    traindata = TD_np[train_index]\n",
    "    testdata = TD_np[test_index]\n",
    "    \n",
    "    #Load the model to be trained (save separately, because we do not want to repeatedly retrain the same model)\n",
    "    nlp1 = deepcopy(nlp)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp1.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    ner.add_label(\"OBJ\")\n",
    "    ner.add_label(\"MON\")\n",
    "    ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp1.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp1.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(traindata)\n",
    "            batches = minibatch(traindata, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp1.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    results = evaluate(nlp1,testdata)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    oblist = list(ev_obj[0].values())\n",
    "    newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data=eval_data.append(newrow1,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow2,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow3,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow4,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow5,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the contents of our evaluation dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ents_p     ents_r     ents_f label\n",
      "0     0.000000   0.000000   0.000000  DATE\n",
      "1    75.000000   3.529412   6.741573   MON\n",
      "2     0.000000   0.000000   0.000000   OBJ\n",
      "3    50.000000   4.000000   7.407407   ORG\n",
      "4    91.111111  81.349206  85.953878   PER\n",
      "5    76.033058  64.335664  69.696970   LOC\n",
      "6     0.000000   0.000000   0.000000  DATE\n",
      "7     0.000000   0.000000   0.000000   MON\n",
      "8     0.000000   0.000000   0.000000   OBJ\n",
      "9   100.000000  15.151515  26.315789   ORG\n",
      "10   83.984375  78.754579  81.285444   PER\n",
      "11   82.517483  67.428571  74.213836   LOC\n",
      "12    0.000000   0.000000   0.000000  DATE\n",
      "13  100.000000   9.090909  16.666667   MON\n",
      "14    0.000000   0.000000   0.000000   OBJ\n",
      "15  100.000000   5.714286  10.810811   ORG\n",
      "16   81.900452  83.796296  82.837529   PER\n",
      "17   85.950413  70.270270  77.323420   LOC\n",
      "18    0.000000   0.000000   0.000000  DATE\n",
      "19  100.000000   3.076923   5.970149   MON\n",
      "20    0.000000   0.000000   0.000000   OBJ\n",
      "21   50.000000   6.779661  11.940299   ORG\n",
      "22   84.656085  81.218274  82.901554   PER\n",
      "23   80.412371  54.929577  65.271967   LOC\n",
      "24    0.000000   0.000000   0.000000  DATE\n",
      "25   17.647059   6.666667   9.677419   MON\n",
      "26    0.000000   0.000000   0.000000   OBJ\n",
      "27   71.428571  17.857143  28.571429   ORG\n",
      "28   78.278689  80.590717  79.417879   PER\n",
      "29   80.451128  67.295597  73.287671   LOC\n"
     ]
    }
   ],
   "source": [
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we can create estimates of performance averaged over all the trials, providing a better estimate of each measurement with its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure mean and standard deviation of f, p and r scores for each label \n",
    "a = eval_data.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f               ents_p                ents_r          \n",
      "            mean       std       mean        std       mean       std\n",
      "label                                                                \n",
      "DATE    0.000000  0.000000   0.000000   0.000000   0.000000  0.000000\n",
      "LOC    71.958773  4.622225  81.072891   3.609012  64.851936  5.930612\n",
      "MON     7.811162  6.071459  58.529412  46.925362   4.472782  3.499442\n",
      "OBJ     0.000000  0.000000   0.000000   0.000000   0.000000  0.000000\n",
      "ORG    17.009147  9.703169  74.285714  25.050968   9.900521  6.183875\n",
      "PER    82.479257  2.407120  83.986142   4.694122  81.141815  1.809540\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the different labels perform consistently at the levels printed above. The PER and LOC labels are perhaps the most useful, whereas the others can still be improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Spelling Normalization\n",
    "\n",
    "We can apply the evaluation above to a model trained with text whose spelling has been normalized, thus evaluating whether the inclusion of a normalization dictionary improves training results.\n",
    "\n",
    "To apply the spelling normalization, we create a pipeline component that modifies the NORM attribute of each token according to a dictionary we provide. Spacy does not modify any text supplied permanently, this is the way they provide for correcting for spelling variation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Norm Exceptions from JSON file\n",
    "with open('normalizeddict.json', 'r', encoding='utf-8') as fp3:\n",
    "    NORM_EXCEPTIONS = json.load(fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These steps are all addressed in more detail in another notebook, \"Adding a Custom Pipeline Component in Spacy\"\n",
    "\n",
    "#Define and add pipeline component that updates .norm attribute\n",
    "\n",
    "def add_custom_norms(doc):\n",
    "    for token in doc:\n",
    "        if token.text in NORM_EXCEPTIONS:\n",
    "            token.norm_ = NORM_EXCEPTIONS[token.text]\n",
    "    return doc\n",
    "\n",
    "#Add component to the pipeline\n",
    "\n",
    "nlp.add_pipe(add_custom_norms, first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a new blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data2 = pd.DataFrame(columns=columns)\n",
    "eval_data2 = eval_data2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ents_p</th>\n",
       "      <th>ents_r</th>\n",
       "      <th>ents_f</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ents_p, ents_r, ents_f, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 36681.309747757856}\n",
      "Losses {'ner': 28037.00775129828}\n",
      "Losses {'ner': 26839.918097875427}\n",
      "Losses {'ner': 21681.210179250193}\n",
      "Losses {'ner': 22691.217211608542}\n",
      "Losses {'ner': 24099.61153436615}\n",
      "Losses {'ner': 24012.12943678908}\n",
      "Losses {'ner': 26742.850501861423}\n",
      "Losses {'ner': 27828.868456542492}\n",
      "Losses {'ner': 28967.126047454774}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 28155.425471669838}\n",
      "Losses {'ner': 23234.60611417357}\n",
      "Losses {'ner': 17871.88060454513}\n",
      "Losses {'ner': 16091.027753552835}\n",
      "Losses {'ner': 16774.26912097751}\n",
      "Losses {'ner': 16791.395927003003}\n",
      "Losses {'ner': 22314.34671662748}\n",
      "Losses {'ner': 24901.671098547056}\n",
      "Losses {'ner': 25097.306005179882}\n",
      "Losses {'ner': 27372.017644405365}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 34922.73215593383}\n",
      "Losses {'ner': 23168.115738138247}\n",
      "Losses {'ner': 20729.01249424047}\n",
      "Losses {'ner': 20437.462174263477}\n",
      "Losses {'ner': 21176.84745838726}\n",
      "Losses {'ner': 24841.752232989296}\n",
      "Losses {'ner': 25889.203417696204}\n",
      "Losses {'ner': 27083.804209765978}\n",
      "Losses {'ner': 28313.46140109212}\n",
      "Losses {'ner': 29566.480840966105}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29131.860930730174}\n",
      "Losses {'ner': 22860.817320672286}\n",
      "Losses {'ner': 22136.24543326888}\n",
      "Losses {'ner': 17300.281348499666}\n",
      "Losses {'ner': 15263.59707153529}\n",
      "Losses {'ner': 12860.694824622837}\n",
      "Losses {'ner': 11344.360642946906}\n",
      "Losses {'ner': 12008.03018742557}\n",
      "Losses {'ner': 11328.806644731507}\n",
      "Losses {'ner': 11477.042313059741}\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Model trained with EMS dictionary\n",
    "\n",
    "for train_index, test_index in split:\n",
    "    \n",
    "    #Generate training and test data\n",
    "    traindata = TD_np[train_index]\n",
    "    testdata = TD_np[test_index]\n",
    "    \n",
    "    #Load the model to be trained (save separately, because we do not want to repeatedly retrain the same model)\n",
    "    nlp2 = deepcopy(nlp)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp2.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    ner.add_label(\"OBJ\")\n",
    "    ner.add_label(\"MON\")\n",
    "    ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp2.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp2.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(traindata)\n",
    "            batches = minibatch(traindata, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp2.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    results = evaluate(nlp2,testdata)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    oblist = list(ev_obj[0].values())\n",
    "    newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data2=eval_data2.append(newrow1,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow2,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow3,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow4,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow5,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= eval_data2.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the statistics for the training with (b) and without (a) spelling normalization. As can be seen, there is a slight improvement on most measurements (as well as a reduction in variability) when we normalize spelling. \n",
    "\n",
    "This measurement shows null performance of the DATE and OBJ labels; this must be reviewed, but may be because of the way the data was shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f               ents_p                ents_r          \n",
      "            mean       std       mean        std       mean       std\n",
      "label                                                                \n",
      "DATE    0.000000  0.000000   0.000000   0.000000   0.000000  0.000000\n",
      "LOC    71.958773  4.622225  81.072891   3.609012  64.851936  5.930612\n",
      "MON     7.811162  6.071459  58.529412  46.925362   4.472782  3.499442\n",
      "OBJ     0.000000  0.000000   0.000000   0.000000   0.000000  0.000000\n",
      "ORG    17.009147  9.703169  74.285714  25.050968   9.900521  6.183875\n",
      "PER    82.479257  2.407120  83.986142   4.694122  81.141815  1.809540\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f                ents_p                ents_r           \n",
      "            mean        std       mean        std       mean        std\n",
      "label                                                                  \n",
      "DATE    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000\n",
      "LOC    75.037776   5.809667  82.289110   4.286447  69.089502   7.240889\n",
      "MON     4.944057   3.993193  54.166667  41.666667   2.590609   2.097429\n",
      "OBJ     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000\n",
      "ORG    25.192883  14.734935  63.179348   9.102998  16.683038  11.137080\n",
      "PER    81.216748   1.827689  81.409103   1.435101  81.046223   2.619820\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
