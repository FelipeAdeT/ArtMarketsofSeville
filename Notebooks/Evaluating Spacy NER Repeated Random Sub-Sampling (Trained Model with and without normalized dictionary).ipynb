{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the results of Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of training (and its evaluation) will depend on how the data was split into training and testing sets. In this worksheet, we use repeated random subsampling to assess the performance of our trained model.\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics)):\n",
    "\n",
    ">This method, also known as Monte Carlo cross-validation,[16] creates multiple random splits of the dataset into training and validation data.[17] For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over k-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (folds). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits.\n",
    "\n",
    "We will be dividing our data into an 80-20 split, using 80% for training and 20% for testing. This will be repeated randomly for each iteration of training to evaluate how much the training improves results on average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish \n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "from collections import defaultdict, Counter\n",
    "from spacy.attrs import ORTH\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.language import GoldParse\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Tagged Data from JSON file\n",
    "with open('TaggedData_SF.json', 'r', encoding='utf-8') as fp2:\n",
    "    TAGGED_DATA = json.load(fp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has a built-in function for evaluating a model's performance using the [command line](https://spacy.io/api/cli#evaluate), but alternatively you can define a function like the one below. It takes the NER model and examples that you input and returns several metrics:\n",
    "        - UAS (Unlabelled Attachment Score) \n",
    "        - LAS (Labelled Attachment Score)\n",
    "        - ents_p\n",
    "        - ents_r\n",
    "        - ents_f\n",
    "        - tags_acc\n",
    "        - token_acc\n",
    "\n",
    "[According](https://github.com/explosion/spaCy/issues/2405) to one of the creators of Spacy, \n",
    ">The UAS and LAS are standard metrics to evaluate dependency parsing. UAS is the proportion of tokens whose head has been correctly assigned, LAS is the proportion of tokens whose head has been correctly assigned with the right dependency label (subject, object, etc).\n",
    ">ents_p, ents_r, ents_f are the precision, recall and fscore for the NER task.\n",
    ">tags_acc is the POS tagging accuracy.\n",
    ">token_acc seems to be the precision for token segmentation.\n",
    "\n",
    "The key metrics for this task are the precision, recall and f-score.\n",
    "**Precision** (ents_p) is the ratio of correctly-labeled entities out of all the entities labeled. (True Positive/(True Positive+False Positive)).\n",
    "**Recall**  (ents_r) is the ratio of correctly-labeled entities out of all true entities (True Positive/(True Positive+False Negative)). The F-score is the mean of both values.  \n",
    "\n",
    "These metrics all appear averaged out through all the entity types (labels) and then detailed for each label in particular. We want these values to be as close as possible to 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def evaluate(ner_model, examples):\n",
    "        scorer = Scorer()\n",
    "        for sents, ents in examples:\n",
    "            doc_gold = ner_model.make_doc(sents)\n",
    "            gold = GoldParse(doc_gold, entities=ents['entities'])\n",
    "            pred_value = ner_model(sents)\n",
    "            scorer.score(pred_value, gold)\n",
    "        return scorer.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the Spacy model, define a blank dataframe to store the output of our different trials, and calculate the amount of data necessary for an 80-20 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy Model\n",
    "nlp= spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data = pd.DataFrame(columns=columns)\n",
    "eval_data = eval_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326.40000000000003"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate 80% of data for an 80-20 split\n",
    "\n",
    "len(TAGGED_DATA)*0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the training loop ten times, each with a different 80-20 split, and store the evaluation statistics of our NER model in our dataframe. We are using a copy of the NLP model because we want the training to start afresh for each set of training data. Otherwise, the model would be trained on all the data including the test data, leading to the model overperforming on the tagged data compared to new samples that we are interested in tagging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 28067.362139642217}\n",
      "Losses {'ner': 26077.473458575434}\n",
      "Losses {'ner': 25732.790645901237}\n",
      "Losses {'ner': 25423.783981692308}\n",
      "Losses {'ner': 24873.045012149792}\n",
      "Losses {'ner': 25221.563670720905}\n",
      "Losses {'ner': 25339.74750509858}\n",
      "Losses {'ner': 25090.522224128246}\n",
      "Losses {'ner': 24895.211273133755}\n",
      "Losses {'ner': 24554.592849495355}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29028.443981474196}\n",
      "Losses {'ner': 26535.32327965144}\n",
      "Losses {'ner': 26292.086815213435}\n",
      "Losses {'ner': 25841.25038647733}\n",
      "Losses {'ner': 25902.866968294635}\n",
      "Losses {'ner': 25656.89452091232}\n",
      "Losses {'ner': 25887.78905776143}\n",
      "Losses {'ner': 25367.45244167745}\n",
      "Losses {'ner': 25995.728346973658}\n",
      "Losses {'ner': 25172.58657830581}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 27290.306967409793}\n",
      "Losses {'ner': 25090.18081061895}\n",
      "Losses {'ner': 24641.981659894846}\n",
      "Losses {'ner': 24662.654106218833}\n",
      "Losses {'ner': 24211.69880866655}\n",
      "Losses {'ner': 23966.144414107664}\n",
      "Losses {'ner': 24124.79556529969}\n",
      "Losses {'ner': 23943.54584768042}\n",
      "Losses {'ner': 23849.92142687738}\n",
      "Losses {'ner': 24112.667848318815}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29341.542794615954}\n",
      "Losses {'ner': 27261.05710970097}\n",
      "Losses {'ner': 26495.96780495113}\n",
      "Losses {'ner': 26205.65187996399}\n",
      "Losses {'ner': 25990.564476336272}\n",
      "Losses {'ner': 25806.83585464582}\n",
      "Losses {'ner': 26052.673229801003}\n",
      "Losses {'ner': 25636.212341895327}\n",
      "Losses {'ner': 26052.60259948671}\n",
      "Losses {'ner': 25656.602459728718}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 30275.82644467696}\n",
      "Losses {'ner': 28098.2131320345}\n",
      "Losses {'ner': 27537.53798761471}\n",
      "Losses {'ner': 27208.02515631076}\n",
      "Losses {'ner': 27252.625558234715}\n",
      "Losses {'ner': 27290.96278436482}\n",
      "Losses {'ner': 27089.783796951175}\n",
      "Losses {'ner': 26991.23956760578}\n",
      "Losses {'ner': 26833.597617149353}\n",
      "Losses {'ner': 26979.624137340114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 28509.36463215958}\n",
      "Losses {'ner': 26050.167337943512}\n",
      "Losses {'ner': 25457.364234812892}\n",
      "Losses {'ner': 25443.007019339377}\n",
      "Losses {'ner': 25178.34919881077}\n",
      "Losses {'ner': 25597.729673262686}\n",
      "Losses {'ner': 25344.103919938672}\n",
      "Losses {'ner': 24958.77422168851}\n",
      "Losses {'ner': 25418.82894744724}\n",
      "Losses {'ner': 25086.66230070591}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 32535.204346468145}\n",
      "Losses {'ner': 30088.455238804494}\n",
      "Losses {'ner': 29453.92096221316}\n",
      "Losses {'ner': 29313.49556240329}\n",
      "Losses {'ner': 29326.282054278767}\n",
      "Losses {'ner': 29167.1843454279}\n",
      "Losses {'ner': 28902.969515618868}\n",
      "Losses {'ner': 29165.487021811306}\n",
      "Losses {'ner': 29344.095430403948}\n",
      "Losses {'ner': 28950.12413278222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 27702.989935073965}\n",
      "Losses {'ner': 26101.388527461655}\n",
      "Losses {'ner': 25277.2713517674}\n",
      "Losses {'ner': 25013.269684964027}\n",
      "Losses {'ner': 25484.252986246778}\n",
      "Losses {'ner': 25022.646217403933}\n",
      "Losses {'ner': 24980.72864601761}\n",
      "Losses {'ner': 25011.730789244175}\n",
      "Losses {'ner': 24909.540938850492}\n",
      "Losses {'ner': 24457.738883562386}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 31080.09514921786}\n",
      "Losses {'ner': 28670.748923293104}\n",
      "Losses {'ner': 27945.224625468247}\n",
      "Losses {'ner': 27965.923443717766}\n",
      "Losses {'ner': 27794.135915773688}\n",
      "Losses {'ner': 27817.828810952604}\n",
      "Losses {'ner': 27758.504013635218}\n",
      "Losses {'ner': 28048.27471022308}\n",
      "Losses {'ner': 27625.66261018254}\n",
      "Losses {'ner': 27580.358522176743}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29640.437973298034}\n",
      "Losses {'ner': 27341.978425757316}\n",
      "Losses {'ner': 26470.14491775891}\n",
      "Losses {'ner': 26183.058625902864}\n",
      "Losses {'ner': 26456.184109028058}\n",
      "Losses {'ner': 26332.198845272884}\n",
      "Losses {'ner': 26281.250217121094}\n",
      "Losses {'ner': 26147.041104391217}\n",
      "Losses {'ner': 25935.223890662193}\n",
      "Losses {'ner': 25969.405787262483}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29561.011135987133}\n",
      "Losses {'ner': 27376.658474487678}\n",
      "Losses {'ner': 26681.567853248638}\n",
      "Losses {'ner': 26967.920826007787}\n",
      "Losses {'ner': 26529.398723179038}\n",
      "Losses {'ner': 26340.85194108216}\n",
      "Losses {'ner': 26422.428576783743}\n",
      "Losses {'ner': 26818.571851305664}\n",
      "Losses {'ner': 26468.83990008384}\n",
      "Losses {'ner': 26221.15822866559}\n"
     ]
    }
   ],
   "source": [
    "# Testing how much the evaluation depends on texts included in testing data\n",
    "\n",
    "#Loop 10 times\n",
    "for x in range(0,10):\n",
    "    \n",
    "    #Batching the Tagged Data into training and evaluation data (80-20 split)\n",
    "\n",
    "    random.shuffle(TAGGED_DATA)\n",
    "    train_data = TAGGED_DATA[:326]\n",
    "    test_data = TAGGED_DATA[326:]\n",
    "\n",
    "    #Load the model to be trained (save separately, because we do not want to repeatedly retrain the same model)\n",
    "    nlp1 = deepcopy(nlp)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp1.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    ner.add_label(\"OBJ\")\n",
    "    ner.add_label(\"MON\")\n",
    "    ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp1.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp1.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp1.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    \n",
    "    #Testing NER results of existing model on test data\n",
    "\n",
    "    results = evaluate(nlp1,test_data)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    oblist = list(ev_obj[0].values())\n",
    "    newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data=eval_data.append(newrow1,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow2,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow3,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow4,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow5,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the contents of our evaluation dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ents_p     ents_r     ents_f label\n",
      "0     0.000000   0.000000   0.000000  DATE\n",
      "1    61.224490  50.000000  55.045872   MON\n",
      "2    50.000000   2.020202   3.883495   OBJ\n",
      "3    55.813953  38.095238  45.283019   ORG\n",
      "4    78.599222  82.786885  80.638723   PER\n",
      "5    85.227273  87.719298  86.455331   LOC\n",
      "6     0.000000   0.000000   0.000000  DATE\n",
      "7    65.000000  63.934426  64.462810   MON\n",
      "8     0.000000   0.000000   0.000000   OBJ\n",
      "9    39.473684  29.411765  33.707865   ORG\n",
      "10   85.507246  85.922330  85.714286   PER\n",
      "11   84.285714  81.379310  82.807018   LOC\n",
      "12    0.000000   0.000000   0.000000  DATE\n",
      "13   80.487805  33.673469  47.482014   MON\n",
      "14  100.000000   1.162791   2.298851   OBJ\n",
      "15   20.930233  16.981132  18.750000   ORG\n",
      "16   94.190871  82.246377  87.814313   PER\n",
      "17   87.837838  77.844311  82.539683   LOC\n",
      "18   33.333333   6.250000  10.526316  DATE\n",
      "19   71.698113  57.575758  63.865546   MON\n",
      "20   40.000000   2.941176   5.479452   OBJ\n",
      "21   60.606061  35.087719  44.444444   ORG\n",
      "22   86.666667  85.903084  86.283186   PER\n",
      "23   83.333333  87.349398  85.294118   LOC\n",
      "24   50.000000   7.142857  12.500000  DATE\n",
      "25   81.481481  50.000000  61.971831   MON\n",
      "26   50.000000   1.666667   3.225806   OBJ\n",
      "27   26.923077  14.000000  18.421053   ORG\n",
      "28   90.776699  89.473684  90.120482   PER\n",
      "29   80.468750  78.030303  79.230769   LOC\n",
      "..         ...        ...        ...   ...\n",
      "36   80.000000  28.571429  42.105263  DATE\n",
      "37   84.615385  66.666667  74.576271   MON\n",
      "38  100.000000   1.408451   2.777778   OBJ\n",
      "39   16.666667   8.888889  11.594203   ORG\n",
      "40   87.647059  86.627907  87.134503   PER\n",
      "41   85.046729  75.833333  80.176211   LOC\n",
      "42   50.000000  11.764706  19.047619  DATE\n",
      "43   68.888889  55.357143  61.386139   MON\n",
      "44  100.000000   2.197802   4.301075   OBJ\n",
      "45   54.545455  45.283019  49.484536   ORG\n",
      "46   87.053571  85.152838  86.092715   PER\n",
      "47   91.608392  81.875000  86.468647   LOC\n",
      "48   66.666667  16.666667  26.666667  DATE\n",
      "49   59.523810  54.347826  56.818182   MON\n",
      "50    0.000000   0.000000   0.000000   OBJ\n",
      "51   41.666667  16.949153  24.096386   ORG\n",
      "52   87.659574  86.554622  87.103594   PER\n",
      "53   90.845070  80.124224  85.148515   LOC\n",
      "54    0.000000   0.000000   0.000000  DATE\n",
      "55   81.250000  45.614035  58.426966   MON\n",
      "56    0.000000   0.000000   0.000000   OBJ\n",
      "57   30.555556  22.916667  26.190476   ORG\n",
      "58   86.175115  82.378855  84.234234   PER\n",
      "59   86.330935  82.191781  84.210526   LOC\n",
      "60   50.000000   7.142857  12.500000  DATE\n",
      "61   60.869565  33.734940  43.410853   MON\n",
      "62    0.000000   0.000000   0.000000   OBJ\n",
      "63   36.666667  24.444444  29.333333   ORG\n",
      "64   86.134454  85.774059  85.953878   PER\n",
      "65   86.986301  78.881988  82.736156   LOC\n",
      "\n",
      "[66 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure mean and standard deviation of f, p and r scores for each label \n",
    "a = eval_data.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f                ents_p                ents_r           \n",
      "            mean        std       mean        std       mean        std\n",
      "label                                                                  \n",
      "DATE   12.425382  13.227116  33.030303  29.267143   7.806532   8.790330\n",
      "LOC    83.570682   2.360039  86.212179   3.180820  81.217373   3.732953\n",
      "MON    58.878322   8.416557  71.064200   9.380229  51.436929  10.623568\n",
      "OBJ     1.996951   2.077158  40.000000  43.588989   1.036099   1.089365\n",
      "ORG    31.989093  13.655120  41.346887  17.342943  26.485795  11.824742\n",
      "PER    86.409244   2.541786  87.188831   3.788883  85.726376   2.600771\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluating Spelling Normalization\n",
    "\n",
    "We can apply the evaluation above to a model trained with text whose spelling has been normalized, thus evaluating whether the inclusion of a normalization dictionary improves training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Norm Exceptions from JSON file\n",
    "with open('normalizeddict.json', 'r', encoding='utf-8') as fp3:\n",
    "    NORM_EXCEPTIONS = json.load(fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "nlp2= spacy.load('es_core_news_md')\n",
    "\n",
    "#Define and add pipeline component that updates .norm attribute\n",
    "\n",
    "def add_custom_norms(doc):\n",
    "    for token in doc:\n",
    "        if token.text in NORM_EXCEPTIONS:\n",
    "            token.norm_ = NORM_EXCEPTIONS[token.text]\n",
    "    return doc\n",
    "\n",
    "#Add component to the pipeline\n",
    "\n",
    "nlp2.add_pipe(add_custom_norms, first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a new blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data2 = pd.DataFrame(columns=columns)\n",
    "eval_data2 = eval_data2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29704.185964004595}\n",
      "Losses {'ner': 27239.500042788895}\n",
      "Losses {'ner': 26941.961559539537}\n",
      "Losses {'ner': 26145.953828093996}\n",
      "Losses {'ner': 26334.20562622226}\n",
      "Losses {'ner': 26604.300484430045}\n",
      "Losses {'ner': 26098.342754028272}\n",
      "Losses {'ner': 26418.023478515446}\n",
      "Losses {'ner': 25914.664099514484}\n",
      "Losses {'ner': 26235.914868056774}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 30703.52576867009}\n",
      "Losses {'ner': 28725.902252893466}\n",
      "Losses {'ner': 28159.463954733743}\n",
      "Losses {'ner': 27959.68541136016}\n",
      "Losses {'ner': 27966.129337390652}\n",
      "Losses {'ner': 27301.701287878677}\n",
      "Losses {'ner': 27441.524910437874}\n",
      "Losses {'ner': 27437.85645264387}\n",
      "Losses {'ner': 27486.76589106745}\n",
      "Losses {'ner': 27322.532069921494}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 30608.731875505335}\n",
      "Losses {'ner': 28358.62261412488}\n",
      "Losses {'ner': 28061.892892574677}\n",
      "Losses {'ner': 27869.223551096162}\n",
      "Losses {'ner': 27870.966918962076}\n",
      "Losses {'ner': 27565.368818713352}\n",
      "Losses {'ner': 27750.462330672424}\n",
      "Losses {'ner': 27679.12888814509}\n",
      "Losses {'ner': 27550.064000189304}\n",
      "Losses {'ner': 27454.43350493908}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 30636.544206978608}\n",
      "Losses {'ner': 28662.39909793232}\n",
      "Losses {'ner': 27990.096906065486}\n",
      "Losses {'ner': 27770.806052751723}\n",
      "Losses {'ner': 28045.94076333518}\n",
      "Losses {'ner': 27441.458400078118}\n",
      "Losses {'ner': 28033.74474290572}\n",
      "Losses {'ner': 27503.52598297596}\n",
      "Losses {'ner': 27305.03563812375}\n",
      "Losses {'ner': 27613.302244063467}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 29822.95023346947}\n",
      "Losses {'ner': 27813.459726089448}\n",
      "Losses {'ner': 27374.243101541037}\n",
      "Losses {'ner': 27357.323377413064}\n",
      "Losses {'ner': 26978.66976794449}\n",
      "Losses {'ner': 26844.57572968828}\n",
      "Losses {'ner': 27291.27380744554}\n",
      "Losses {'ner': 27105.986064648023}\n",
      "Losses {'ner': 27108.813636779785}\n",
      "Losses {'ner': 27120.995013475418}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 31271.56411062826}\n",
      "Losses {'ner': 29240.612178586984}\n",
      "Losses {'ner': 28423.878584734834}\n",
      "Losses {'ner': 28382.362940905965}\n",
      "Losses {'ner': 28346.81670781219}\n",
      "Losses {'ner': 28201.512787211686}\n",
      "Losses {'ner': 28134.756526775658}\n",
      "Losses {'ner': 28543.895895455033}\n",
      "Losses {'ner': 28476.213165938854}\n",
      "Losses {'ner': 28297.5568472445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 31785.638530219283}\n",
      "Losses {'ner': 29261.247385079063}\n",
      "Losses {'ner': 29008.69970434797}\n",
      "Losses {'ner': 28910.39984441354}\n",
      "Losses {'ner': 28785.794552255975}\n",
      "Losses {'ner': 28358.168015688658}\n",
      "Losses {'ner': 28308.551798445405}\n",
      "Losses {'ner': 28462.10376200825}\n",
      "Losses {'ner': 28321.0605584383}\n",
      "Losses {'ner': 28554.489853855222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 30592.462978096773}\n",
      "Losses {'ner': 28048.832299594138}\n",
      "Losses {'ner': 27656.271471435477}\n",
      "Losses {'ner': 27689.566155280103}\n",
      "Losses {'ner': 27565.01172095465}\n",
      "Losses {'ner': 27682.711843401194}\n",
      "Losses {'ner': 27400.2855032878}\n",
      "Losses {'ner': 27560.821975003928}\n",
      "Losses {'ner': 27233.499860771}\n",
      "Losses {'ner': 27121.40555819869}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 30191.85820032967}\n",
      "Losses {'ner': 28012.922388982814}\n",
      "Losses {'ner': 27230.52329029574}\n",
      "Losses {'ner': 27153.121265769354}\n",
      "Losses {'ner': 27153.325990892423}\n",
      "Losses {'ner': 27121.976557270857}\n",
      "Losses {'ner': 26927.481867267983}\n",
      "Losses {'ner': 27001.31964278221}\n",
      "Losses {'ner': 27125.673155542463}\n",
      "Losses {'ner': 26938.167551059276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 31167.908232477508}\n",
      "Losses {'ner': 28852.424275443653}\n",
      "Losses {'ner': 28278.796518534036}\n",
      "Losses {'ner': 28049.632881542348}\n",
      "Losses {'ner': 27915.398711656686}\n",
      "Losses {'ner': 27721.87783415802}\n",
      "Losses {'ner': 27537.995221124962}\n",
      "Losses {'ner': 27828.64074844122}\n",
      "Losses {'ner': 27496.009865987115}\n",
      "Losses {'ner': 27907.930468946695}\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Model trained with EMS dictionary\n",
    "\n",
    "#Loop 10 times\n",
    "for x in range(0,10):\n",
    "    \n",
    "    random.shuffle(TAGGED_DATA)\n",
    "    train_data = TAGGED_DATA[:326]\n",
    "    test_data = TAGGED_DATA[326:]\n",
    "    \n",
    "    #Load the model to be trained\n",
    "    nlp3 = deepcopy(nlp2)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp3.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    ner.add_label(\"OBJ\")\n",
    "    ner.add_label(\"MON\")\n",
    "    ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp3.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp3.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp3.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "   \n",
    " #Testing NER results of existing model on test data\n",
    "\n",
    "    results = evaluate(nlp3,test_data)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    oblist = list(ev_obj[0].values())\n",
    "    newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data2=eval_data2.append(newrow1,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow2,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow3,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow4,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow5,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= eval_data2.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the statistics for the training with (b) and without (a) spelling normalization. As can be seen, there is a slight improvement on most measurements (as well as a reduction in variability) when we normalize spelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f                ents_p                ents_r           \n",
      "            mean        std       mean        std       mean        std\n",
      "label                                                                  \n",
      "DATE   12.425382  13.227116  33.030303  29.267143   7.806532   8.790330\n",
      "LOC    83.570682   2.360039  86.212179   3.180820  81.217373   3.732953\n",
      "MON    58.878322   8.416557  71.064200   9.380229  51.436929  10.623568\n",
      "OBJ     1.996951   2.077158  40.000000  43.588989   1.036099   1.089365\n",
      "ORG    31.989093  13.655120  41.346887  17.342943  26.485795  11.824742\n",
      "PER    86.409244   2.541786  87.188831   3.788883  85.726376   2.600771\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f                ents_p                ents_r          \n",
      "            mean        std       mean        std       mean       std\n",
      "label                                                                 \n",
      "DATE   18.951137  14.983290  55.000000  39.907300  11.682644  9.683086\n",
      "LOC    82.371995   3.815829  87.378347   4.921542  78.021471  4.186711\n",
      "MON    57.162421   8.427770  66.077303   6.438534  50.754813  9.905893\n",
      "OBJ     4.121914   5.276423  51.500000  46.070598   2.213534  2.899095\n",
      "ORG    24.539355  10.168417  31.333530  10.121525  20.720597  9.793148\n",
      "PER    87.089760   3.189638  90.179737   3.023785  84.264011  4.038805\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
