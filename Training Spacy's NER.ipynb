{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Spacy's Named Entity Recognizer (NER)\n",
    "\n",
    "In this document, we train Spacy's NER model on our own documents to improve results for our particular texts, with the possibility of adding new labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why train?\n",
    "\n",
    "Spacy's statistical models are trained on different corpora, often specific genres like news or Wikipedia articles.  Depending on your own documents, the structures of these genres may not map onto those of your own (for instance, if you are using posts from social media, or in our case, historical texts). Training on your own data can improve accuracy for existing tags.\n",
    "\n",
    "An alternative to improving an existent model is training your own from scratch, but this necessitates much larger amounts of data; whereas Spacy recommends a few hundred to a few thousand examples for updating an existing model, you might need up to a million to train a new one. \n",
    "\n",
    "# Necessary Inputs\n",
    "\n",
    "To train a preexisting model, we need the model you want to work with and a corpus of tagged texts that will be divided into training and test data. \n",
    "\n",
    "### A Pretrained Spacy Model\n",
    "\n",
    "First, we load the predefined model that we are interested in using, here the Spanish medium-sized model. Check [spacy.io](https://spacy.io/usage/models) for models in your language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we import modules to be used in this session\n",
    "\n",
    "### must go over these and see if any are unnecessary. ###\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish \n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "from collections import defaultdict, Counter\n",
    "from spacy.attrs import ORTH\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.language import GoldParse\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we load the Spacy Model to be trained\n",
    "\n",
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how the model performs out-of-the-box, we upload a short text, process it with the Spanish model (resulting in a 'doc' object) and view the Named Entity Recognition results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process a single text.\n",
    "\n",
    "doc1=nlp(\"antonio de alfian pintor vezino de esta cibdad de seuilla en la collacion de la madalena otorgo e conozco que doy todo mi poder a francisco de arfian mi hijo especialmente para que por mi pueda pedir de alonso de la milla clerigo mayordomo de la fabrica de la iglesia de santa maria de la villa de carmona sesenta e cinco ducados que me a de pagar por virtud de un mandamiento del señor provisor\")\n",
    "\n",
    "#View out-of-the-box NLP results with displacy\n",
    "\n",
    "displacy.render(doc1, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Displacysingletext](https://user-images.githubusercontent.com/57915371/69157731-62b9f800-0ab3-11ea-8055-539a8028b9a8.png \"(Displacy does not render on Github, so this is an image of the output from the commands above.)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'antonio de alfian pintor vezino de esta cibdad de seuilla en la collacion de la madalena otorgo e conozco que doy todo mi poder a francisco de arfian mi hijo especialmente para que por mi pueda pedir de alonso de la milla clerigo mayordomo de la fabrica de la iglesia de santa maria de la villa de carmona sesenta e cinco ducados que me a de pagar por virtud de un mandamiento del señor provisor'\n",
      "PER antonio de alfian\n",
      "MISC la collacion de la madalena otorgo\n",
      "PER francisco de arfian\n",
      "PER alonso de la milla clerigo mayordomo\n",
      "LOC iglesia de santa maria de la villa de carmona\n"
     ]
    }
   ],
   "source": [
    "#Alternatively, you could create a list of entities:\n",
    "print(\"Entities in '%s'\" % doc1)\n",
    "for ent in doc1.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it does some good work already, but there are several mistakes. The third PER (person) tag  picks up the name of the individual, but also some irrelevant titles. The LOC tag picks up a church within a city, which we would prefer to have as an ORG, and it missed the city of Seville (seuilla) in the first row. We would like to improve the accuracy of these preexisting tags. There is also a monetary quantity (sesenta e cinco ducados) that we would like the model to identify. You might be interested in adding new tags to a preexisting model. The Spacy medium Spanish model includes tags for PER (person), LOC (location), ORG (organization) and MISC (miscellaneous) entities. We will add MON (monetary), DATE and OBJ (object) tags.\n",
    "\n",
    "### Training and Test Data\n",
    "\n",
    "Training requires that you have your own annotated data for the training of the model itself and later evaluation. For NER, this means that you need a set of texts for which you have correctly labeled entities. This is largely a manual process, made easier by softwares designed for data tagging. [Prodigy](https://prodi.gy) is Spacy's own software, but open-access alternatives are also available ([Dataturks](http://dataturks.com), [Brat](http://brat.nlplab.org), [Tagtog](http://tagtog.net)). \n",
    "\n",
    "You should aspire to perfect data tagging. This apparently simple goal will actually require some thought, as the edges of entities are often unclear. It is helpful to ask yourself: if this were the model's output, would I be happy with the result? Make entities as short as possible, and tag related entities separately (their relationships can be modeled after). Tags must not overlap (take special care if using Dataturks, because their automated selection of similar words, though helpful overall, often causes this issue). If several people are doing the tagging, make sure you share a common set of criteria.\n",
    "\n",
    "We have used [Dataturks](https://dataturks.com) for elaborating our own training data (downloadable here). It is open-source and results in a format easily importable into Spacy. To speed the tagging process, we first ran the Spacy model on our untagged texts, generating a prior annotated version (with many errors) that we uploaded onto Dataturks (instructions here). We then edited the tags and exported the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Dataturks training data to Spacy Format\n",
    "\n",
    "Both Dataturks and Spacy work with JSON files, but they are formatted slightly differently. \n",
    "\n",
    "We first define a function for converting the Dataturks data into the Spacy format (available [here](https://dataturks.com/help/dataturks-ner-json-to-spacy-train.php))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function that converts Dataturks into Spacy format:\n",
    "\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply this function to our own data, saving the object as **TAGGED_DATA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Dataturks to Spacy Format\n",
    "\n",
    "TAGGED_DATA = convert_dataturks_to_spacy(\"/Users/Felipe/Documents/Research/NPL/SevillianPaintersNPL/seville painters test 2-3.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a Python object that is formatted for training the model. But first, we can more formally measure how well the model was performing without any further training.\n",
    "\n",
    "# Evaluating Out-of-the-Box Results\n",
    "\n",
    "Spacy has a built-in function for evaluating a model's performance using the [command line](https://spacy.io/api/cli#evaluate), but alternatively you can define a function like the one below. It takes the NER model and examples that you input and returns several metrics:\n",
    "        - UAS (Unlabelled Attachment Score) \n",
    "        - LAS (Labelled Attachment Score)\n",
    "        - ents_p\n",
    "        - ents_r\n",
    "        - ents_f\n",
    "        - tags_acc\n",
    "        - token_acc\n",
    "\n",
    "[According](https://github.com/explosion/spaCy/issues/2405) to one of the creators of Spacy, \n",
    ">The UAS and LAS are standard metrics to evaluate dependency parsing. UAS is the proportion of tokens whose head has been correctly assigned, LAS is the proportion of tokens whose head has been correctly assigned with the right dependency label (subject, object, etc).\n",
    ">ents_p, ents_r, ents_f are the precision, recall and fscore for the NER task.\n",
    ">tags_acc is the POS tagging accuracy.\n",
    ">token_acc seems to be the precision for token segmentation.\n",
    "\n",
    "The key metrics for this task are the precision, recall and f-score.\n",
    "**Precision** (ents_p) is the ratio of correctly-labeled entities out of all the entities labeled. (True Positive/(True Positive+False Positive)).\n",
    "**Recall**  (ents_r) is the ratio of correctly-labeled entities out of all true entities (True Positive/(True Positive+False Negative)). The F-score is the mean of both values.  \n",
    "\n",
    "These metrics all appear averaged out through all the entity types (labels) and then detailed for each label in particular. We want these values to be as close as possible to 100. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the evaluation function\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for sents, ents in examples:\n",
    "        doc_gold = ner_model.make_doc(sents)\n",
    "        gold = GoldParse(doc_gold, entities=ents['entities'])\n",
    "        pred_value = ner_model(sents)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uas': 0.0, 'las': 0.0, 'ents_p': 46.5510881593508, 'ents_r': 42.24974891195179, 'ents_f': 44.296244296244296, 'ents_per_type': {'PER': {'p': 54.290931223371885, 'r': 75.91489361702128, 'f': 63.307310149041875}, 'LOC': {'p': 41.92439862542955, 'r': 47.71838331160365, 'f': 44.634146341463406}, 'ORG': {'p': 8.695652173913043, 'r': 1.5037593984962405, 'f': 2.564102564102564}, 'OBJ': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'MON': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'DATE': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'tags_acc': 0.0, 'token_acc': 100.0, 'textcat_score': 0.0, 'textcats_per_cat': {}}\n"
     ]
    }
   ],
   "source": [
    "#Applying the evaluation function to pre-trained model with all our tagged data\n",
    "\n",
    "results = evaluate(nlp,TAGGED_DATA)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the overall precision, recall and f-score are low (in the 40s), but this is largely due to the fact that we have tags for labels that don't yet exist in the model (for all these new labels, the model scores 0 on all these measures). For existing labels, we see that results vary, but can really be improved. \n",
    "\n",
    "# Training the Model\n",
    "\n",
    "As we said before, we will need some data for training and some for evaluation. Typically, an 80-20 split is a good choice (80% training data, 20% test data).  \n",
    "\n",
    "We first calculate just how many data points we need for training, based on the size of our tagged sample. Then we randomly split our tagged data proportionally into train_data and test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326.40000000000003\n"
     ]
    }
   ],
   "source": [
    "#Calculate 80% of tagged data for division into training and testing data\n",
    "\n",
    "print(len(TAGGED_DATA)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching the Tagged Data into training and evaluation data (80-20)\n",
    "\n",
    "random.shuffle(TAGGED_DATA)\n",
    "train_data = TAGGED_DATA[:326]\n",
    "test_data = TAGGED_DATA[326:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the training process. Spacy provides a way to train the model via the [command line](https://spacy.io/usage/training#spacy-train-cli), but you can also build the training loop manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model to be trained\n",
    "nlp1 = nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 28975.20223899102}\n",
      "Losses {'ner': 26519.306228797388}\n",
      "Losses {'ner': 26266.936020213427}\n",
      "Losses {'ner': 25969.679946707794}\n",
      "Losses {'ner': 25953.834477057186}\n",
      "Losses {'ner': 25745.2414760245}\n",
      "Losses {'ner': 25725.066081359982}\n",
      "Losses {'ner': 25396.63461640291}\n",
      "Losses {'ner': 25465.391352713108}\n",
      "Losses {'ner': 25519.021299855784}\n"
     ]
    }
   ],
   "source": [
    "#Create object for retrieving the NER pipeline component\n",
    "ner=nlp1.get_pipe(\"ner\")\n",
    "\n",
    "#Generate new labels for the NER component (if you wish to create new labels)\n",
    "ner.add_label(\"OBJ\")\n",
    "ner.add_label(\"MON\")\n",
    "ner.add_label(\"DATE\")\n",
    "\n",
    "#This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "with nlp1.disable_pipes('tagger','parser'):\n",
    "#Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "    optimizer= nlp1.resume_training()\n",
    "#Would need to figure this out, they are the sizes for the minibatching\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "#This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "    for itn in range(10):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp1.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Updated Model\n",
    "\n",
    "Training the model is that simple. If you want to use the updated model in the future, you will want to save it to a directory you have created beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's change the model's metadata to reflect that it is a new model that has been trained. It will allow us to recall it independently from the old one in future uses.\n",
    "nlp.meta['name']='core_news_EM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save trained model to disk for later reuse\n",
    "\n",
    "nlp.to_disk(\"Documents/Research/NPL/SevillianPaintersNPL/EM Spanish Model/Trained_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy recommends that you package the model, making it easier to employ after. This must be done on the command line. First, call the directory your model is in. Then, following the lines of code below:\n",
    "\n",
    "Create the directory in which your packaged model will go.\n",
    "\n",
    "Spacy's package command creates a model package directory.\n",
    "\n",
    "Call the directory in which the .tar file will be stored.\n",
    "\n",
    "Build the package, creating a .tar.gz file in the directory /dist.\n",
    "\n",
    "Install the model by pointing pip to the file path.\n",
    "\n",
    "```mkdir PackagedModel\n",
    "python -m spacy package Trained_Model PackagedModel\n",
    "cd PackagedModel/es_core_news_md-2.2.0/\n",
    "python setup.py sdist\n",
    "cd dist/\n",
    "pip install es_core_news_md-2.2.0.tar.gz\n",
    "```\n",
    "\n",
    "Next time you use spacy, you will be able to call the updated model by using the **spacy.load()** command with the model name.\n",
    "\n",
    "\n",
    "# Evaluating the Trained Model\n",
    "\n",
    "Now that the model has been updated, you can check how much it has improved. Visually, using our previous example, we can see that the tagging has improved dramatically on preexisting and new labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2=nlp1(\"antonio de alfian pintor vezino de esta cibdad de seuilla en la collacion de la madalena otorgo e conozco que doy todo mi poder a francisco de arfian mi hijo especialmente para que por mi pueda pedir de alonso de la milla clerigo mayordomo de la fabrica de la iglesia de santa maria de la villa de carmona sesenta e cinco ducados que me a de pagar por virtud de un mandamiento del señor provisor\")\n",
    "\n",
    "#View out-of-the-box NLP results with displacy\n",
    "\n",
    "displacy.render(doc2, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can more formally measure this by using the test_data with the evaluate function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Displacysingletexttrained](https://user-images.githubusercontent.com/57915371/69158379-54201080-0ab4-11ea-9cd1-ccb52469c87c.png \"(Displacy does not render on Github, so this is an image of the output from the commands above.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uas': 0.0, 'las': 0.0, 'ents_p': 88.90577507598785, 'ents_r': 88.23529411764706, 'ents_f': 88.56926570779713, 'ents_per_type': {'LOC': {'p': 89.28571428571429, 'r': 92.5925925925926, 'f': 90.9090909090909}, 'PER': {'p': 98.25174825174825, 'r': 96.89655172413794, 'f': 97.56944444444444}, 'OBJ': {'p': 41.7910447761194, 'r': 44.44444444444444, 'f': 43.07692307692307}, 'ORG': {'p': 86.66666666666667, 'r': 81.25, 'f': 83.87096774193549}, 'DATE': {'p': 87.5, 'r': 53.84615384615385, 'f': 66.66666666666667}, 'MON': {'p': 97.10144927536231, 'r': 94.36619718309859, 'f': 95.71428571428571}}, 'tags_acc': 0.0, 'token_acc': 100.0, 'textcat_score': 0.0, 'textcats_per_cat': {}}\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test/evaluation data\n",
    "\n",
    "results2 = evaluate(nlp1,test_data)\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the results have improved greatly for preexisting labels (PER, ORG, LOC) and have improved for the new labels (DATE, MON, OBJ). \n",
    "\n",
    "The extent to which you can improve your model will depend on several parameters, including the quantity and quality of training data. It is worth noting that the measurements above will to some extent depend on the ways the data was shuffled; so one iteration may perform better or worse depending on which texts were taken as training data. We have created a notebook in which you can run several trials and develop an idea of the average values of these indicators and their variance. \n",
    "\n",
    "It is also worth noting that texts can have different characteristics that will affect the performance of the model. As previously stated, models are often trained on modern texts of very defined genres, like news or wikipedia articles. Your documents may have characteristics, such as spelling variation, differences in tokenization or differences in sentence fragmentation that could differ from the texts the model was originally trained on. This is especially true of historical documents and those taken from social media. Normalizing your own documents could improve performance. We have another document that helps you include a normalization dictionary in your pipeline before training, and similar adaptations could be made to the tokenizer and sentence fragmenter.\n",
    "\n",
    "Because NER will not provide perfect results, it could be useful to be able to read through all documents with their labels. You can do this with the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize predicted entities in all documents. Displacy does not render on Github, so \n",
    "\n",
    "#Perhaps modify this to use JSON already here instead of importing a new TXT?\n",
    "\n",
    "#df = pd.read_csv('~/Documents/Research/NPL/SevillianPaintersNPL/DocumentsTest.txt', delimiter =\",\")\n",
    "#docs = list(nlp1.pipe(df['document']))\n",
    "#for doc in docs:\n",
    "#    displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "#    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
